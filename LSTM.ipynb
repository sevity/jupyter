{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sshot 2017-08-01 PM 8.48.46.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 NN에서 히든 노드간 연결 추가\n",
    "\n",
    "학습이 아닌 prediction과정에서도 이전 결과가 영향을 줌\n",
    "\n",
    "(학습의 경우로 한정하면 NN의 경우도 이전 결과가 영향을 준다고 할수는 있지만(잘못된 골짜기로 빠지는 등) NN은 prediction의 경우 input 순서에 전혀 무관함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "![](sshot 2017-08-01 PM 8.53.35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: Long Short Term Memory(1997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanishing gradient 문제를 해결하기 위해 고안\n",
    "\n",
    "![](sshot 2017-08-01 PM 8.55.33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell state 추가\n",
    "\n",
    "히든노드간 연결에 영향을 줌 (덧셈 연결이 있어 vanishing gradient문제에 강함)\n",
    "![](sshot 2017-08-01 PM 9.01.43.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forget gate\n",
    "\n",
    "시그모이드 거치고 나서 0~1사이값으로 잊을지 말지 결정. 0이면 잊는거 1이면 안 잊는것\n",
    "\n",
    "![](sshot 2017-08-01 PM 9.03.58.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input gate\n",
    "\n",
    "어떤 정보를 기억할지 결정\n",
    "\n",
    "C't가 기존 RNN에서의 정보에 해당 (cell state candidate)\n",
    "\n",
    "이걸 어느정도 반영할지를 input gate로 계산해서 더해줌\n",
    "\n",
    "![](sshot 2017-08-01 PM 9.05.55.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output gate\n",
    "ht로 필터링된 아웃풋을 내보냄\n",
    "\n",
    "output gate로 아웃풋 강도를 0~1사이로 조절하고, tanh로 cell state값을 -1 ~ +1 사이로 조절해서 곱해서 내보냄\n",
    "\n",
    "![](sshot 2017-08-01 PM 9.11.42.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU: Gated Recurrent Unit(2014)\n",
    "LSTM을 경량화 한 버전 \n",
    "\n",
    "cell state를 따로 두지 않고 hidden state와 통합\n",
    "\n",
    "forget gate와 input gate를 update gate로 통합\n",
    "\n",
    "여전히 덧셈이라는 핵심특성은 유지\n",
    "\n",
    "![](sshot 2017-08-01 PM 9.15.05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM에서 vanishing gradient 문제가 해결 된 이유\n",
    "\n",
    "https://curt-park.github.io/2017-04-03/why-is-lstm-strong-on-gradient-vanishing/\n",
    "\n",
    "대략적으로 덧셈으로 인해 편미분 항목이 단순해짐 (곱셈이었다면 남아있을 항목들이 없어짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
